## Запуск системы

Запуск всех необходимых элементов системы производится командой:

> docker compose up -d --build

Данная команда запустит контейнеры с S3 хранилищем Minio, базой данных Postgres и пайплайном Nifi с registry и zookeeper для сохранения собранных пайплайнов.

После запуска всех контейнеров может потребоваться 5-10 минут, пока nifi полностью загрузится. 

Nifi будет доступен по адресу localhost:8091/nifi
Консоль S3 хранилища будет доступна по адресу localhost:9001. Данные для входа в консоль можно найти в файле .env


## S3 хранилище

В хранилище уже создан bucket testbucket и загружен файл test.csv для воспроизведения результата. Вы можете загрузить свой файл, в таком случае необходимо будет изменить Object Key в процессоре FetchS3Object в Nifi

## Pipeline 

На момент запуска системы все необходимые процессоры в Nifi уже запущены. Если вы загрузили тестовый csv файл с названием, отличным от test.csv, необходимо изменить Object Key в процессоре FetchS3Object, предварительно остановив его. После внесения изменений процессор необходимо запустить вновь. 

Для запуска пайплайна необходимо вызвать команду Run once процессора GenerateFlowFile

По завершении работы пайплайна в таблице public.test базы данных появятся данные из csv файла.

## DDL-команды для создания таблиц в PostgreSQL

Команды для создания таблиц содержатся в файле initial_tables.sql

## Заполнение таблиц тестовыми данными

Для заполнения таблиц тестовыми данными в базе создана хранимая процедура fill_tables(n_rows integer)

Данная процедура забирает n_rows записей из таблицы test, и записывает их в таблицы c, m,t , после чего, удаляет n_rows записей из таблицы test

Пример вызова процедуры для переноса 3000 записей 

> CALL fill_tables(3000)

Скрипт создания процедуры можно найти в файле initial_tables.sql

## Получение агрегатных значений

Для получения агрегатных значений создана хранимая процедура: 

get_aggregates(<br />tablename varchar(50), <br />agg_column agg_column, <br />date_start varchar(20), <br />date_end varchar(20), <br />age_start integer, <br />age_end integer, <br />gender gen, <br />start_sum integer, <br />end_sum integer,<br /> mcc_id integer)

Все параметры данной процедуры являются опциональными:

> tablename - название таблицы, в которую будет записан результат<br />
> agg_column - колонка, по которой будет проводится агрегация. Возможные значения -<br />
> ('gender', 'mcc_id', 'age', 'month', 'year','amount') по умолчанию агрегация
> ведётся по полу 'gender'<br />
> date_start, date_end - начальная и конечная дата транзакции<br />
> age_start, age_end - начальный и конечный возраст для группировки<br />
> start_sum, end_sum - начальная и конечная сумма для группировки<br />
> gender - пол<br />
> mcc_id - отрасль<br />

Пример вызова процедуры для получения агрегированных значений по возрасту за 2022 год с суммами транзакций от 1000 до 3000, с записью в таблицу aggr_age_2022_1000_3000:

> call get_aggregates(tablename => 'aggr_age_2022_1000_3000', agg_column => 'age', 
> date_start => '2022-01-01', date_end => '2022-12-21', start_sum => 1000, end_sum =>3000)

## Генерация файла с тестовыми данными

В файле .env необходимо задать переменные окружения: 

> N_CLIENTS - количество клиентов<br />
> N_TRANSACTIONS - количество транзакций<br />
> N_MERCHANTS - количество торговых точек<br />
> FILENAME - название итогового файла<br />

После этого необходимо запустить скрипт генерации файла коммандой:

> source generate_test_csv.sh

В результате работы скрипта в директории проекта появится файл с $FILENAME, который можно будет использовать в работе пайплайна.